{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWI2TWdrmD_e"
      },
      "source": [
        "# Connect to Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "zdt5C5w3_Pf3"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqYHhiBCmN1k"
      },
      "source": [
        "# Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ybUgDiMQF1R",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "! pip install lpips"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b5XgHIzuGR5l"
      },
      "outputs": [],
      "source": [
        "# Standard imports\n",
        "import random\n",
        "import os\n",
        "import re\n",
        "import glob\n",
        "import time\n",
        "from pathlib import Path\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import tqdm\n",
        "from typing import List, Tuple, Dict\n",
        "\n",
        "\n",
        "# PyTorch imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingLR\n",
        "from torchvision import transforms\n",
        "import torchvision.models as models\n",
        "from PIL import Image\n",
        "import lpips\n",
        "\n",
        "print(\"All imports successful!\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8byFGiMB3PQ"
      },
      "source": [
        "# Data Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tWa8kl7qhcQu"
      },
      "outputs": [],
      "source": [
        "# !unzip -q -o \"/content/drive/MyDrive/CS230/PlantDS_Merged.zip\" -d \"/content/drive/MyDrive/CS230\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lODlNWL87zly"
      },
      "outputs": [],
      "source": [
        "def scan_available_plants(data_root):\n",
        "\n",
        "    path = Path(data_root)\n",
        "    # find all plantXXXXX_day01.png\n",
        "    files = list(path.glob(\"plant*_day*.png\"))\n",
        "    plants = list(path.glob(\"plant*_day01.png\"))\n",
        "\n",
        "    plant_ids = []\n",
        "    for f in plants:\n",
        "        try:\n",
        "            pid = int(f.name.split('_')[0].replace('plant', ''))\n",
        "            plant_ids.append(pid)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    plant_ids.sort()\n",
        "    print(f\"Found {len(files)} images\")\n",
        "    print(f\"Found {len(plant_ids)} plants\")\n",
        "    return plant_ids\n",
        "\n",
        "\n",
        "class Config:\n",
        "    DATA_ROOT = \"/content/drive/MyDrive/CS230/PlantDS_Merged/Seg\"\n",
        "\n",
        "    # Config\n",
        "    IMG_SIZE = 128\n",
        "    BATCH_SIZE = 32\n",
        "    NUM_WORKERS = 2\n",
        "\n",
        "    INPUT_DAYS = [1, 2, 3, 4, 5]\n",
        "    TARGET_DAY = 10\n",
        "\n",
        "    ALL_PLANTS = scan_available_plants(DATA_ROOT)\n",
        "\n",
        "    if len(ALL_PLANTS) == 0:\n",
        "        print(f\"WARNING: No Data in {DATA_ROOT}.\")\n",
        "        TRAIN_PLANTS = []\n",
        "        VAL_PLANTS = []\n",
        "        TEST_PLANTS = []\n",
        "    else:\n",
        "        # Seed\n",
        "        random.seed(42)\n",
        "        shuffled = ALL_PLANTS.copy()\n",
        "        random.shuffle(shuffled)\n",
        "\n",
        "        # 90% Train, 5% Val, 5% Test\n",
        "        n_total = len(shuffled)\n",
        "        n_train = 1850\n",
        "        n_val = 95\n",
        "\n",
        "        TRAIN_PLANTS = shuffled[925:n_train]\n",
        "        VAL_PLANTS = shuffled[n_train : n_train + n_val]\n",
        "        TEST_PLANTS = shuffled[n_train + n_val:]\n",
        "\n",
        "        print(f\"Data Structure:\")\n",
        "        print(f\"   - Total:  {n_total}\")\n",
        "        print(f\"   - Train: {len(TRAIN_PLANTS)}\")\n",
        "        print(f\"   - Valid: {len(VAL_PLANTS)}\")\n",
        "        print(f\"   - Test: {len(TEST_PLANTS)}\")\n",
        "\n",
        "\n",
        "class PlantGrowthDataset(Dataset):\n",
        "    def __init__(self, data_root, plant_ids, input_days=[1,2,3,4,5],\n",
        "                 target_day=10, img_size=128):\n",
        "        self.data_root = Path(data_root)\n",
        "        self.plant_ids = plant_ids\n",
        "        self.input_days = input_days\n",
        "        self.target_day = target_day\n",
        "        self.img_size = img_size\n",
        "        self.transform = self._get_transforms()\n",
        "\n",
        "    def _get_transforms(self):\n",
        "\n",
        "        common_transforms = [\n",
        "            transforms.Resize((self.img_size, self.img_size)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "        ]\n",
        "\n",
        "\n",
        "        return transforms.Compose(common_transforms)\n",
        "\n",
        "    def _get_image_path(self, plant_id, day):\n",
        "        filename = f\"plant{plant_id:05d}_day{day:02d}.png\"\n",
        "        return self.data_root / filename\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.plant_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        plant_id = self.plant_ids[idx]\n",
        "\n",
        "        # Day 1-5\n",
        "        input_images = []\n",
        "        for day in self.input_days:\n",
        "            img_path = self._get_image_path(plant_id, day)\n",
        "            if not img_path.exists():\n",
        "                raise FileNotFoundError(f\"Missing image: {img_path}\")\n",
        "\n",
        "            img = Image.open(img_path).convert('RGB')\n",
        "            img_tensor = self.transform(img)\n",
        "            input_images.append(img_tensor)\n",
        "\n",
        "        input_images = torch.stack(input_images, dim=0) # [5, 3, H, W]\n",
        "\n",
        "        # Day 10\n",
        "        target_path = self._get_image_path(plant_id, self.target_day)\n",
        "        if not target_path.exists():\n",
        "             raise FileNotFoundError(f\"Missing target: {target_path}\")\n",
        "\n",
        "        target_img = Image.open(target_path).convert('RGB')\n",
        "        target_image = self.transform(target_img)\n",
        "\n",
        "        return input_images, target_image, plant_id\n",
        "\n",
        "\n",
        "def create_dataloaders(config):\n",
        "\n",
        "    train_dataset = PlantGrowthDataset(\n",
        "        config.DATA_ROOT, config.TRAIN_PLANTS, config.INPUT_DAYS,\n",
        "        config.TARGET_DAY, config.IMG_SIZE\n",
        "    )\n",
        "    val_dataset = PlantGrowthDataset(\n",
        "        config.DATA_ROOT, config.VAL_PLANTS, config.INPUT_DAYS,\n",
        "        config.TARGET_DAY, config.IMG_SIZE\n",
        "    )\n",
        "    test_dataset = PlantGrowthDataset(\n",
        "        config.DATA_ROOT, config.TEST_PLANTS, config.INPUT_DAYS,\n",
        "        config.TARGET_DAY, config.IMG_SIZE\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=config.BATCH_SIZE,\n",
        "                             shuffle=True, num_workers=config.NUM_WORKERS,\n",
        "                             pin_memory=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=config.BATCH_SIZE,\n",
        "                           shuffle=False, num_workers=config.NUM_WORKERS,\n",
        "                           pin_memory=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=config.BATCH_SIZE,\n",
        "                            shuffle=False, num_workers=config.NUM_WORKERS,\n",
        "                            pin_memory=True)\n",
        "\n",
        "    return train_loader, val_loader, test_loader\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    try:\n",
        "        config = Config()\n",
        "        if len(config.TRAIN_PLANTS) > 0:\n",
        "            train_loader, val_loader, test_loader = create_dataloaders(config)\n",
        "\n",
        "            inputs, targets, ids = next(iter(train_loader))\n",
        "            print(\"\\nDataLoader Test Success!\")\n",
        "            print(f\"Inputs Shape: {inputs.shape} (Batch, Days, Channels, H, W)\")\n",
        "            print(f\"Targets Shape: {targets.shape}\")\n",
        "            print(f\"Plant IDs: {ids.tolist()}\")\n",
        "\n",
        "\n",
        "            def denorm(x): return x * 0.5 + 0.5\n",
        "            plt.figure(figsize=(10, 4))\n",
        "            plt.subplot(1, 2, 1)\n",
        "            plt.imshow(denorm(inputs[0, 0]).permute(1, 2, 0).clip(0, 1))\n",
        "            plt.title(f\"Plant {ids[0]}: Day 1 (Input)\")\n",
        "            plt.axis('off')\n",
        "\n",
        "            plt.subplot(1, 2, 2)\n",
        "            plt.imshow(denorm(targets[0]).permute(1, 2, 0).clip(0, 1))\n",
        "            plt.title(f\"Plant {ids[0]}: Day 10 (Target)\")\n",
        "            plt.axis('off')\n",
        "            plt.show()\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "\n",
        "data_config = Config()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6u0otc8Znsbu"
      },
      "outputs": [],
      "source": [
        "# Test Loading\n",
        "\n",
        "inputs, targets, plant_ids = next(iter(train_loader))\n",
        "\n",
        "print(f\"\\nSuccessfully loaded batch\")\n",
        "print(f\"   Input shape: {inputs.shape}\")\n",
        "print(f\"   Target shape: {targets.shape}\")\n",
        "print(f\"   Plant IDs: {plant_ids.tolist()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXgrpX8CCKX1"
      },
      "source": [
        "# Training Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "npTwK4jAHymS"
      },
      "outputs": [],
      "source": [
        "#  Configuration\n",
        "\n",
        "class TrainConfig:\n",
        "    EPOCHS = 40\n",
        "    LEARNING_RATE = 5e-6\n",
        "    WEIGHT_DECAY = 5e-7\n",
        "\n",
        "    LPIPS_WEIGHT = 1\n",
        "    L1_WEIGHT = 1.\n",
        "    PERCEPTUAL_WEIGHT = 0\n",
        "    PSNR_WEIGHT = 0.00000001\n",
        "    BACKGROUND_WEIGHT = 2.\n",
        "    COLOR_WEIGHT = 1.\n",
        "\n",
        "    USE_MASK = {\n",
        "        'lpips': False,\n",
        "        'l1': False,\n",
        "        'perc': False,\n",
        "        'psnr': False,\n",
        "        'bg': True\n",
        "    }\n",
        "    MASK_THRESHOLD = -0.9\n",
        "\n",
        "    USE_SCHEDULER = True\n",
        "    SCHEDULER_TYPE = 'plateau'\n",
        "    PATIENCE = 10\n",
        "\n",
        "    SAVE_DIR = \"/content/drive/MyDrive/CS230/PlantDS_1_checkpoints\"\n",
        "    SAVE_EVERY = 5\n",
        "\n",
        "    VIS_DIR = \"/content/drive/MyDrive/CS230/PlantDS_1_visualizations\"\n",
        "    VIS_EVERY = 5\n",
        "    NUM_VIS_SAMPLES = 4\n",
        "\n",
        "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    USE_AMP = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "mzmuYXNN1RX1"
      },
      "outputs": [],
      "source": [
        "# Mask Extraction Functions\n",
        "\n",
        "def compute_mask(image, threshold=-0.9):\n",
        "\n",
        "    image_mean = image.mean(dim=1, keepdim=True)\n",
        "    mask = (image_mean > threshold).float()\n",
        "\n",
        "    return mask\n",
        "\n",
        "print(\"Verifying mask computation...\")\n",
        "\n",
        "inputs, targets, _ = next(iter(train_loader))\n",
        "\n",
        "mask = compute_mask(targets[0:1])\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "# Original\n",
        "target_vis = ((targets[0] + 1) / 2).permute(1, 2, 0).cpu().numpy()\n",
        "axes[0].imshow(np.clip(target_vis, 0, 1))\n",
        "axes[0].set_title('Original Target')\n",
        "axes[0].axis('off')\n",
        "\n",
        "# Computed mask\n",
        "mask_vis = mask[0, 0].cpu().numpy()\n",
        "axes[1].imshow(mask_vis, cmap='gray')\n",
        "axes[1].set_title('Computed Mask (White=Plant, Black=Background)')\n",
        "axes[1].axis('off')\n",
        "\n",
        "# Masked\n",
        "target_masked = (targets[0:1] * mask)[0]\n",
        "target_masked_vis = ((target_masked + 1) / 2).permute(1, 2, 0).cpu().numpy()\n",
        "axes[2].imshow(np.clip(target_masked_vis, 0, 1))\n",
        "axes[2].set_title('Masked Target (Only Plant)')\n",
        "axes[2].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Mask coverage: {mask.mean().item()*100:.1f}% of image\")\n",
        "print(\"Mask computation verified. Losses will only be computed on plant regions.\")\n",
        "\n",
        "print(\"Checking mask stability...\")\n",
        "\n",
        "for i, (inputs, targets, _) in enumerate(train_loader):\n",
        "    mask = compute_mask(targets)\n",
        "    coverage = mask.mean().item()\n",
        "    mask_sum = mask.sum().item()\n",
        "\n",
        "    print(f\"Batch {i}: Coverage={coverage*100:.1f}%, Mask sum={mask_sum:.0f}\")\n",
        "\n",
        "    if i >= 3:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9BxS3MqFIDKI"
      },
      "outputs": [],
      "source": [
        "# Loss Functions\n",
        "\n",
        "class LPIPSLoss(nn.Module):\n",
        "\n",
        "    def __init__(self, net='alex', device='cuda', use_mask=True):\n",
        "\n",
        "        super().__init__()\n",
        "        self.use_mask = use_mask\n",
        "        self.lpips_model = lpips.LPIPS(net=net).to(device)\n",
        "\n",
        "        # Freeze parameters\n",
        "        for param in self.lpips_model.parameters():\n",
        "            param.requires_grad = False\n",
        "        self.device = device\n",
        "\n",
        "    def apply_mask_to_image(self, image, mask):\n",
        "\n",
        "        if mask.shape[1] == 1:\n",
        "            mask = mask.expand_as(image)\n",
        "\n",
        "        masked_image = image * mask + (-1.0) * (1 - mask)\n",
        "        return masked_image\n",
        "\n",
        "    def forward(self, pred, target, mask=None):\n",
        "\n",
        "        if self.use_mask and mask is not None:\n",
        "            pred_input = self.apply_mask_to_image(pred, mask)\n",
        "            target_input = self.apply_mask_to_image(target, mask)\n",
        "        else:\n",
        "            pred_input = pred\n",
        "            target_input = target\n",
        "\n",
        "        # LPIPS expects images in [-1, 1] range\n",
        "        # Compute LPIPS distance\n",
        "        lpips_distance = self.lpips_model(pred_input, target_input)\n",
        "\n",
        "        return lpips_distance.mean()\n",
        "\n",
        "\n",
        "class PerceptualLoss(nn.Module):\n",
        "# Perceptual loss using VGG16\n",
        "\n",
        "    def __init__(self, device='cuda', use_mask=TrainConfig.USE_MASK['perc']):\n",
        "        super().__init__()\n",
        "        vgg = models.vgg16(weights='DEFAULT').features.eval().to(device)\n",
        "        for param in vgg.parameters():\n",
        "            param.requires_grad = False\n",
        "        self.feature_layers = [3, 8, 15, 22]\n",
        "        self.vgg = vgg\n",
        "        self.criterion = nn.L1Loss()\n",
        "        self.use_mask = use_mask\n",
        "\n",
        "        self.mean = torch.tensor([0.485, 0.456, 0.406], device=device).view(1, 3, 1, 1)\n",
        "        self.std = torch.tensor([0.229, 0.224, 0.225], device=device).view(1, 3, 1, 1)\n",
        "\n",
        "    def normalize_vgg(self, x):\n",
        "        x = (x + 1) / 2\n",
        "        return (x - self.mean) / self.std\n",
        "\n",
        "    def apply_mask_to_image(self, image, mask):\n",
        "    # Apply mask to image before VGG\n",
        "        if mask.shape[1] == 1:\n",
        "            mask = mask.expand_as(image)\n",
        "\n",
        "        mean_rgb = torch.tensor([0.485, 0.456, 0.406], device=image.device).view(1, 3, 1, 1)\n",
        "        masked_image = image * mask + mean_rgb * (1 - mask)\n",
        "        return masked_image\n",
        "\n",
        "    def forward(self, pred, target, mask=None):\n",
        "        if self.use_mask and mask is not None:\n",
        "            pred_input = self.apply_mask_to_image(pred, mask)\n",
        "            target_input = self.apply_mask_to_image(target, mask)\n",
        "        else:\n",
        "            pred_input = pred\n",
        "            target_input = target\n",
        "\n",
        "        pred_vgg = self.normalize_vgg(pred_input)\n",
        "        target_vgg = self.normalize_vgg(target_input)\n",
        "\n",
        "        loss = 0.0\n",
        "        for i, layer in enumerate(self.vgg):\n",
        "            pred_vgg = layer(pred_vgg)\n",
        "            target_vgg = layer(target_vgg)\n",
        "\n",
        "            if i in self.feature_layers:\n",
        "                loss += self.criterion(pred_vgg, target_vgg)\n",
        "\n",
        "        return loss\n",
        "\n",
        "\n",
        "class PSNRLoss(nn.Module):\n",
        "\n",
        "    def __init__(self, max_val=2.0, use_mask=TrainConfig.USE_MASK['psnr']):\n",
        "        super().__init__()\n",
        "        self.max_val = max_val\n",
        "        self.use_mask = use_mask\n",
        "\n",
        "    def forward(self, pred, target, mask=None):\n",
        "        if self.use_mask and mask is not None:\n",
        "\n",
        "            pred_masked = pred * mask\n",
        "            target_masked = target * mask\n",
        "            mse = torch.sum((pred_masked - target_masked) ** 2) / (torch.sum(mask) * pred.shape[1] + 1e-10)\n",
        "        else:\n",
        "            mse = torch.mean((pred - target) ** 2)\n",
        "\n",
        "        mse = torch.clamp(mse, min=1e-10)\n",
        "        psnr = 20 * torch.log10(self.max_val / torch.sqrt(mse))\n",
        "        return -psnr\n",
        "\n",
        "\n",
        "class BackgroundRegularizationLoss(nn.Module):\n",
        "# Penalize non-black predictions in background regions\n",
        "\n",
        "    def __init__(self, target_value=-1.0):\n",
        "        super().__init__()\n",
        "        self.target_value = target_value\n",
        "\n",
        "    def forward(self, pred, mask):\n",
        "\n",
        "        background_mask = torch.ones_like(mask, device=pred.device) - mask\n",
        "\n",
        "        target = torch.full_like(pred, self.target_value, device=pred.device)\n",
        "\n",
        "        background_error = (pred - target) ** 2\n",
        "\n",
        "        masked_error = background_error * background_mask\n",
        "\n",
        "        num_background_pixels = background_mask.sum() * pred.shape[1]\n",
        "\n",
        "        if num_background_pixels > 0:\n",
        "            loss = masked_error.sum() / num_background_pixels\n",
        "        else:\n",
        "            loss = torch.tensor(0.0, device=pred.device)\n",
        "\n",
        "        return loss\n",
        "\n",
        "\n",
        "class ColorDifferenceLoss(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.criterion = nn.MSELoss(reduction='none')\n",
        "\n",
        "\n",
        "    def forward(self, pred, target, mask):\n",
        "\n",
        "        if mask is None:\n",
        "            # No mask, use full image\n",
        "            return self.criterion(pred, target).mean()\n",
        "\n",
        "        # Expand mask to 3 channels\n",
        "        if mask.shape[1] == 1:\n",
        "            mask_3ch = mask.expand_as(pred)\n",
        "        else:\n",
        "            mask_3ch = mask\n",
        "\n",
        "        # Compute color difference\n",
        "        color_diff = self.criterion(pred, target)\n",
        "\n",
        "        # Apply mask and compute mean over masked pixels\n",
        "        masked_diff = color_diff * mask_3ch\n",
        "\n",
        "        # Count masked pixels (3 channels)\n",
        "        num_masked_pixels = mask_3ch.sum()\n",
        "\n",
        "        if num_masked_pixels > 0:\n",
        "            loss = masked_diff.sum() / num_masked_pixels\n",
        "        else:\n",
        "            # Fallback if mask is empty\n",
        "            loss = color_diff.mean()\n",
        "\n",
        "        return loss\n",
        "\n",
        "\n",
        "class CombinedLoss(nn.Module):\n",
        "\n",
        "    def __init__(self, lpips_weight=1., l1_weight=0, perceptual_weight=0.,\n",
        "                 psnr_weight=0., background_weight=1., color_weight=1.,\n",
        "                 device='cuda', use_mask={'lpips':False, 'l1':False, 'perc':False, 'psnr':False, 'bg':True}):\n",
        "        super().__init__()\n",
        "        self.lpips_weight = lpips_weight\n",
        "        self.l1_weight = l1_weight\n",
        "        self.perceptual_weight = perceptual_weight\n",
        "        self.psnr_weight = psnr_weight\n",
        "        self.background_weight = background_weight\n",
        "        self.color_weight = color_weight\n",
        "        self.use_mask = use_mask\n",
        "\n",
        "        self.lpips_loss = LPIPSLoss(net='alex', device=device, use_mask=use_mask['lpips']) if lpips_weight > 0 else None\n",
        "        self.l1_loss = nn.L1Loss(reduction='none') # if l1_weight > 0 else None\n",
        "        self.perceptual_loss = PerceptualLoss(device, use_mask['perc']) if perceptual_weight > 0 else None\n",
        "        self.psnr_loss = PSNRLoss(use_mask=use_mask['psnr']) if psnr_weight > 0 else None\n",
        "        self.background_loss = BackgroundRegularizationLoss() if background_weight > 0 else None\n",
        "        self.color_loss = ColorDifferenceLoss() if color_weight > 0 else None\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        # Compute mask\n",
        "        mask = compute_mask(target)\n",
        "        mask_coverage = mask.sum() / mask.numel()\n",
        "\n",
        "        if mask_coverage < 0.01:\n",
        "            mask = None\n",
        "\n",
        "        # L1 Loss with masking\n",
        "        l1 = self.l1_loss(pred, target)\n",
        "        if self.use_mask['l1'] and mask is not None:\n",
        "            l1 = (l1 * mask).sum() / (mask.sum() * pred.shape[1] + 1e-10)\n",
        "        else:\n",
        "            l1 = l1.mean()\n",
        "\n",
        "        # VGG Perceptual Loss\n",
        "        if self.perceptual_loss:\n",
        "            perc = self.perceptual_loss(pred, target, mask)\n",
        "        else:\n",
        "            perc = torch.tensor(0.0, device=pred.device)\n",
        "\n",
        "        # LPIPS Loss\n",
        "\n",
        "        if self.lpips_loss:\n",
        "            lpips = self.lpips_loss(pred, target, mask)\n",
        "        else:\n",
        "            lpips = torch.tensor(0.0, device=pred.device)\n",
        "\n",
        "        # PSNR Loss\n",
        "        if self.psnr_loss:\n",
        "            psnr = self.psnr_loss(pred, target, mask)\n",
        "        else:\n",
        "            psnr = torch.tensor(0.0, device=pred.device)\n",
        "\n",
        "        # Background Loss\n",
        "        if self.background_loss and self.use_mask['bg'] and mask is not None:\n",
        "            bg = self.background_loss(pred, mask)\n",
        "        else:\n",
        "            bg = torch.tensor(0.0, device=pred.device)\n",
        "\n",
        "        # Color Difference Loss\n",
        "        if self.color_loss and self.use_mask['bg'] and mask is not None:\n",
        "            color = self.color_loss(pred, target, mask)\n",
        "        else:\n",
        "            color = torch.tensor(0.0, device=pred.device)\n",
        "\n",
        "        # Combined Loss\n",
        "        total = (self.lpips_weight * lpips +\n",
        "                self.l1_weight * l1 +\n",
        "                self.perceptual_weight * perc +\n",
        "                self.psnr_weight * psnr +\n",
        "                self.background_weight * bg +\n",
        "                self.color_weight * color)\n",
        "\n",
        "        return total, lpips, l1, perc, psnr, bg, color"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZKaAPNIIIIDk"
      },
      "outputs": [],
      "source": [
        "# Training Functions\n",
        "\n",
        "def calculate_psnr(pred, target):\n",
        "# Calculate Peak Signal-to-Noise Ratio\n",
        "    mse = torch.mean((pred - target) ** 2)\n",
        "\n",
        "    if mse < 1e-10:\n",
        "        return torch.tensor(100.0, device=pred.device)\n",
        "\n",
        "    psnr = 20 * torch.log10(2.0 / torch.sqrt(mse))\n",
        "    return psnr\n",
        "\n",
        "\n",
        "def train_one_epoch(model, loader, criterion, optimizer, scaler, device, epoch):\n",
        "\n",
        "    model.train()\n",
        "    total_loss, total_lpips, total_l1, total_psnr, total_bg, total_color = 0, 0, 0, 0, 0, 0\n",
        "    pbar = tqdm(loader, desc=f\"Epoch {epoch} [Train]\")\n",
        "\n",
        "    for inputs, targets, _ in pbar:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with torch.amp.autocast('cuda', enabled=scaler is not None):\n",
        "            outputs = model(inputs)\n",
        "            loss, lpips_loss, l1_loss, perc_loss, psnr_loss, bg_loss, color_loss = criterion(outputs, targets)\n",
        "\n",
        "        if scaler:\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.unscale_(optimizer)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_lpips += lpips_loss.item()\n",
        "        total_l1 += l1_loss.item()\n",
        "        total_psnr += psnr_loss.item()\n",
        "        total_bg += bg_loss.item()\n",
        "        total_color += color_loss.item()\n",
        "\n",
        "        pbar.set_postfix({\n",
        "            'total loss': f'{loss.item():.4f}',\n",
        "            'lpips': f'{lpips_loss.item():.4f}',\n",
        "            'l1': f'{l1_loss.item():.4f}',\n",
        "            'perc': f'{perc_loss.item():.4f}',\n",
        "            'psnr': f'{-psnr_loss.item():.4f}',\n",
        "            'bg': f'{bg_loss.item():.4f}',\n",
        "            'color': f'{color_loss.item():.4f}'\n",
        "            })\n",
        "\n",
        "    return (total_loss/len(loader), total_lpips/len(loader), total_l1/len(loader),\n",
        "            total_psnr/len(loader), total_bg/len(loader), total_color/len(loader))\n",
        "\n",
        "def validate(model, loader, criterion, device, epoch):\n",
        "\n",
        "    model.eval()\n",
        "    total_loss, total_lpips, total_l1, total_psnr, total_bg, total_color = 0, 0, 0, 0, 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets, _ in tqdm(loader, desc=f\"Epoch {epoch} [Val]\"):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss, lpips_loss, l1_loss, perc_loss, psnr_loss, bg_loss, color_loss = criterion(outputs, targets)\n",
        "            psnr = calculate_psnr(outputs, targets)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            total_lpips += lpips_loss.item()\n",
        "            total_l1 += l1_loss.item()\n",
        "            total_psnr += psnr_loss.item()\n",
        "            total_bg += bg_loss.item()\n",
        "            total_color += color_loss.item()\n",
        "\n",
        "    return (total_loss/len(loader), total_lpips/len(loader), total_l1/len(loader),\n",
        "            total_psnr/len(loader), total_bg/len(loader), total_color/len(loader))\n",
        "\n",
        "\n",
        "def save_checkpoint(model, optimizer, scheduler, epoch, history, config, is_best=False):\n",
        "\n",
        "    checkpoint = {\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n",
        "        'history': history,\n",
        "    }\n",
        "    save_dir = Path(config.SAVE_DIR)\n",
        "    save_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    torch.save(checkpoint, save_dir / 'checkpoint_latest.pth')\n",
        "    if is_best:\n",
        "        torch.save(checkpoint, save_dir / 'checkpoint_best.pth')\n",
        "        print(f\"Best checkpoint saved!\")\n",
        "    if epoch % config.SAVE_EVERY == 0:\n",
        "        torch.save(checkpoint, save_dir / f'checkpoint_epoch_{epoch:03d}.pth')\n",
        "\n",
        "print(\"Training functions defined!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HK0tHLp1IKEP",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Visualization Function\n",
        "\n",
        "def denormalize(tensor):\n",
        "    return (tensor + 1) / 2\n",
        "\n",
        "\n",
        "def visualize_predictions(model, loader, device, epoch, save_dir, num_samples=4):\n",
        "    model.eval()\n",
        "    inputs, targets, plant_ids = next(iter(loader))\n",
        "    inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        predictions = model(inputs)\n",
        "\n",
        "    # Randomly select samples from the batch\n",
        "    batch_size = inputs.shape[0]\n",
        "    num_samples = min(num_samples, batch_size)\n",
        "    random_indices = torch.randperm(batch_size)[:num_samples]\n",
        "\n",
        "    fig, axes = plt.subplots(num_samples, 7, figsize=(21, 3*num_samples))\n",
        "    if num_samples == 1:\n",
        "        axes = axes.reshape(1, -1)\n",
        "\n",
        "    for idx, i in enumerate(random_indices):\n",
        "        for j in range(5):\n",
        "            img = denormalize(inputs[i,j]).permute(1,2,0).cpu().numpy()\n",
        "            axes[idx,j].imshow(np.clip(img, 0, 1))\n",
        "            axes[idx,j].set_title(f'Day {j+1}')\n",
        "            axes[idx,j].axis('off')\n",
        "\n",
        "        pred = denormalize(predictions[i]).permute(1,2,0).cpu().numpy()\n",
        "        axes[idx,5].imshow(np.clip(pred, 0, 1))\n",
        "        axes[idx,5].set_title('Predicted', color='blue')\n",
        "        axes[idx,5].axis('off')\n",
        "\n",
        "        target = denormalize(targets[i]).permute(1,2,0).cpu().numpy()\n",
        "        axes[idx,6].imshow(np.clip(target, 0, 1))\n",
        "        axes[idx,6].set_title('Ground Truth', color='green')\n",
        "        axes[idx,6].axis('off')\n",
        "\n",
        "    plt.suptitle(f'Epoch {epoch}', fontsize=14, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "\n",
        "    save_path = Path(save_dir) / f'predictions_epoch_{epoch:03d}.png'\n",
        "    save_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "print(\"Visualization functions defined!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KbYFS2ujIPVn"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, val_loader, config):\n",
        "\n",
        "    device = config.DEVICE\n",
        "    model = model.to(device)\n",
        "\n",
        "    criterion = CombinedLoss(\n",
        "        config.LPIPS_WEIGHT,\n",
        "        config.L1_WEIGHT,\n",
        "        config.PERCEPTUAL_WEIGHT,\n",
        "        config.PSNR_WEIGHT,\n",
        "        config.BACKGROUND_WEIGHT,\n",
        "        config.COLOR_WEIGHT,\n",
        "        device,\n",
        "        use_mask=config.USE_MASK\n",
        "    )\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=config.LEARNING_RATE, weight_decay=config.WEIGHT_DECAY)\n",
        "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5,patience=config.PATIENCE)\n",
        "    scaler = torch.cuda.amp.GradScaler() if config.USE_AMP else None\n",
        "\n",
        "    history = {\n",
        "        'train_loss': [], 'val_loss': [],\n",
        "        'train_lpips': [], 'train_l1': [], 'train_psnr': [], 'train_bg': [], 'train_color': [],\n",
        "        'val_lpips': [], 'val_l1': [], 'val_psnr': [], 'val_bg': [], 'val_color': [],\n",
        "        'lr': []\n",
        "    }\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    print(f\"\\nTraining on {device}\")\n",
        "    print(f\"Epochs: {config.EPOCHS} | LR: {config.LEARNING_RATE}\")\n",
        "\n",
        "    for epoch in range(1, config.EPOCHS + 1):\n",
        "        print(f\"\\n{'='*60}\\nEpoch {epoch}/{config.EPOCHS}\")\n",
        "\n",
        "        train_loss, train_lpips, train_l1, train_psnr, train_bg, train_color = train_one_epoch(\n",
        "            model, train_loader, criterion, optimizer, scaler, device, epoch\n",
        "        )\n",
        "        val_loss, val_lpips, val_l1, val_psnr, val_bg, val_color = validate(\n",
        "            model, val_loader, criterion, device, epoch\n",
        "        )\n",
        "\n",
        "        old_lr = optimizer.param_groups[0]['lr']\n",
        "        scheduler.step(val_loss)\n",
        "        new_lr = optimizer.param_groups[0]['lr']\n",
        "        if old_lr != new_lr:\n",
        "            print(f\"Learning rate reduced: {old_lr:.2e} -> {new_lr:.2e}\")\n",
        "\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['train_lpips'].append(train_lpips)\n",
        "        history['train_l1'].append(train_l1)\n",
        "        history['train_psnr'].append(-train_psnr)\n",
        "        history['train_bg'].append(train_bg)\n",
        "        history['train_color'].append(train_color)\n",
        "\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['val_lpips'].append(val_lpips)\n",
        "        history['val_l1'].append(val_l1)\n",
        "        history['val_psnr'].append(val_psnr)\n",
        "        history['val_bg'].append(val_bg)\n",
        "        history['val_color'].append(val_color)\n",
        "\n",
        "        history['lr'].append(optimizer.param_groups[0]['lr'])\n",
        "\n",
        "        print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
        "        print(f\"Train PSNR: {train_psnr:.2f}dB |Val PSNR: {val_psnr:.2f}dB \")\n",
        "\n",
        "        is_best = val_loss < best_val_loss\n",
        "        if is_best:\n",
        "            best_val_loss = val_loss\n",
        "\n",
        "        save_checkpoint(model, optimizer, scheduler, epoch, history, config, is_best)\n",
        "\n",
        "        if epoch % config.VIS_EVERY == 0:\n",
        "            visualize_predictions(model, val_loader, device, epoch,\n",
        "                                config.VIS_DIR, config.NUM_VIS_SAMPLES)\n",
        "\n",
        "    print(f\"\\nTraining complete! Best val loss: {best_val_loss:.4f}\")\n",
        "\n",
        "    return model, history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3L5gN1D5I2s1"
      },
      "outputs": [],
      "source": [
        "def plot_training_history(history, save_path=None):\n",
        "# Plot training and validation losses over epochs including LPIPS\n",
        "\n",
        "    import matplotlib.pyplot as plt\n",
        "    import numpy as np\n",
        "\n",
        "    epochs = range(1, len(history['train_loss']) + 1)\n",
        "\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(20, 10))\n",
        "\n",
        "    # Plot 1: Total Loss\n",
        "    axes[0, 0].plot(epochs, history['train_loss'], 'b-', label='Train Loss', linewidth=2)\n",
        "    axes[0, 0].plot(epochs, history['val_loss'], 'r-', label='Val Loss', linewidth=2)\n",
        "    axes[0, 0].set_xlabel('Epoch')\n",
        "    axes[0, 0].set_ylabel('Loss')\n",
        "    axes[0, 0].set_title('Total Loss')\n",
        "    axes[0, 0].legend()\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot 2: PSNR\n",
        "    valid_inverted = [-x for x in history['val_psnr']]\n",
        "    axes[0, 1].plot(epochs, valid_inverted, label='Valid', linewidth=2)\n",
        "    axes[0, 1].plot(epochs, history['train_psnr'], label='Train', linewidth=2)\n",
        "    axes[0, 1].set_xlabel('Epoch')\n",
        "    axes[0, 1].set_ylabel('PSNR (dB)')\n",
        "    axes[0, 1].set_title('PSNR')\n",
        "    axes[0, 1].legend()\n",
        "    axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot 3: L1\n",
        "    axes[0, 2].plot(epochs, history['val_l1'], label='Valid', linewidth=2)\n",
        "    axes[0, 2].plot(epochs, history['train_l1'], label='Train', linewidth=2)\n",
        "    axes[0, 2].set_xlabel('Epoch')\n",
        "    axes[0, 2].set_ylabel('L1')\n",
        "    axes[0, 2].set_title('L1')\n",
        "    axes[0, 2].legend()\n",
        "    axes[0, 2].grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot 4: Individual Train Loss Components\n",
        "    axes[1, 1].plot(epochs, history['train_l1'], label='L1 Loss', linewidth=2)\n",
        "    axes[1, 1].plot(epochs, history['train_color'], label='Color Loss', linewidth=2)\n",
        "    axes[1, 1].plot(epochs, history['train_bg'], label='Background Loss', linewidth=2)\n",
        "    axes[1, 1].plot(epochs, history['train_lpips'], label='LPIPS Loss', linewidth=2)\n",
        "    axes[1, 1].set_xlabel('Epoch')\n",
        "    axes[1, 1].set_ylabel('Loss')\n",
        "    axes[1, 1].set_title('Training Loss Components')\n",
        "    axes[1, 1].legend()\n",
        "    axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot 5: Individual Valid Loss Components\n",
        "    axes[1, 0].plot(epochs, history['val_l1'], label='L1 Loss', linewidth=2)\n",
        "    axes[1, 0].plot(epochs, history['val_color'], label='Color Loss', linewidth=2)\n",
        "    axes[1, 0].plot(epochs, history['val_bg'], label='Background Loss', linewidth=2)\n",
        "    axes[1, 0].plot(epochs, history['val_lpips'], label='LPIPS Loss', linewidth=2)\n",
        "    axes[1, 0].set_xlabel('Epoch')\n",
        "    axes[1, 0].set_ylabel('Loss')\n",
        "    axes[1, 0].set_title('Validation Loss Components')\n",
        "    axes[1, 0].legend()\n",
        "    axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot 6: Learning Rate\n",
        "    axes[1, 2].plot(epochs, history['lr'], 'purple', linewidth=2)\n",
        "    axes[1, 2].set_xlabel('Epoch')\n",
        "    axes[1, 2].set_ylabel('Learning Rate')\n",
        "    axes[1, 2].set_title('Learning Rate Schedule')\n",
        "    axes[1, 2].set_yscale('log')\n",
        "    axes[1, 2].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
        "        print(f\"Plot saved to {save_path}\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    # Print summary statistics\n",
        "    print(\"\\nTraining Summary:\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Initial Train Loss: {history['train_loss'][0]:.4f}\")\n",
        "    print(f\"Final Train Loss: {history['train_loss'][-1]:.4f}\")\n",
        "    print(f\"Best Train Loss: {min(history['train_loss']):.4f} (Epoch {np.argmin(history['train_loss'])+1})\")\n",
        "    print()\n",
        "    print(f\"Initial Val Loss: {history['val_loss'][0]:.4f}\")\n",
        "    print(f\"Final Val Loss: {history['val_loss'][-1]:.4f}\")\n",
        "    print(f\"Best Val Loss: {min(history['val_loss']):.4f} (Epoch {np.argmin(history['val_loss'])+1})\")\n",
        "\n",
        "    if 'val_psnr' in history:\n",
        "        print()\n",
        "        print(f\"Initial Val PSNR: {history['val_psnr'][0]:.2f} dB\")\n",
        "        print(f\"Final Val PSNR: {history['val_psnr'][-1]:.2f} dB\")\n",
        "        print(f\"Best Val PSNR: {max(history['val_psnr']):.2f} dB (Epoch {np.argmax(history['val_psnr'])+1})\")\n",
        "\n",
        "    if 'val_lpips' in history:  # NEW\n",
        "        print()\n",
        "        print(f\"Initial Val LPIPS: {history['val_lpips'][0]:.4f}\")\n",
        "        print(f\"Final Val LPIPS: {history['val_lpips'][-1]:.4f}\")\n",
        "        print(f\"Best Val LPIPS: {min(history['val_lpips']):.4f} (Epoch {np.argmin(history['val_lpips'])+1})\")\n",
        "\n",
        "    print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k24hObwaXgFo"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qd_Wa4xgOKSE"
      },
      "source": [
        "# 2D U-Net Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5NGvKo7YH-Ob"
      },
      "outputs": [],
      "source": [
        "class DoubleConv(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.double_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, 3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, 3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.double_conv(x)\n",
        "\n",
        "\n",
        "class Down(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.maxpool_conv = nn.Sequential(\n",
        "            nn.MaxPool2d(2),\n",
        "            DoubleConv(in_channels, out_channels)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.maxpool_conv(x)\n",
        "\n",
        "\n",
        "class Up(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
        "        super().__init__()\n",
        "        if bilinear:\n",
        "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "            self.conv = DoubleConv(in_channels, out_channels)\n",
        "        else:\n",
        "            self.up = nn.ConvTranspose2d(in_channels, in_channels//2, 2, stride=2)\n",
        "            self.conv = DoubleConv(in_channels, out_channels)\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        x1 = self.up(x1)\n",
        "        diffY = x2.size()[2] - x1.size()[2]\n",
        "        diffX = x2.size()[3] - x1.size()[3]\n",
        "        x1 = F.pad(x1, [diffX//2, diffX-diffX//2, diffY//2, diffY-diffY//2])\n",
        "        x = torch.cat([x2, x1], dim=1)\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class UNet(nn.Module):\n",
        "# U-Net for temporal plant growth prediction\n",
        "    def __init__(self, n_channels=15, n_classes=3, bilinear=True):\n",
        "        super().__init__()\n",
        "        self.n_channels = n_channels\n",
        "        self.n_classes = n_classes\n",
        "\n",
        "        self.inc = DoubleConv(n_channels, 64)\n",
        "        self.down1 = Down(64, 128)\n",
        "        self.down2 = Down(128, 256)\n",
        "        self.down3 = Down(256, 512)\n",
        "        self.down4 = Down(512, 1024 // (2 if bilinear else 1))\n",
        "\n",
        "        self.up1 = Up(1024, 512 // (2 if bilinear else 1), bilinear)\n",
        "        self.up2 = Up(512, 256 // (2 if bilinear else 1), bilinear)\n",
        "        self.up3 = Up(256, 128 // (2 if bilinear else 1), bilinear)\n",
        "        self.up4 = Up(128, 64, bilinear)\n",
        "        self.outc = nn.Conv2d(64, n_classes, 1)\n",
        "        self.output_activation = nn.Tanh()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Reshape: [B, 5, 3, H, W] -> [B, 15, H, W]\n",
        "        B, T, C, H, W = x.shape\n",
        "        x = x.view(B, T * C, H, W)\n",
        "\n",
        "        # Encoder\n",
        "        x1 = self.inc(x)\n",
        "        x2 = self.down1(x1)\n",
        "        x3 = self.down2(x2)\n",
        "        x4 = self.down3(x3)\n",
        "        x5 = self.down4(x4)\n",
        "\n",
        "        # Decoder\n",
        "        x = self.up1(x5, x4)\n",
        "        x = self.up2(x, x3)\n",
        "        x = self.up3(x, x2)\n",
        "        x = self.up4(x, x1)\n",
        "\n",
        "        # Output\n",
        "        logits = self.outc(x)\n",
        "        return self.output_activation(logits)\n",
        "\n",
        "    def count_parameters(self):\n",
        "        total = sum(p.numel() for p in self.parameters())\n",
        "        trainable = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "        return total, trainable\n",
        "\n",
        "print(\"U-Net model defined!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tW8tHWG-ISek",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Create configurations\n",
        "train_config = TrainConfig()\n",
        "\n",
        "# Create dataloaders\n",
        "print(\"\\n Loading data...\")\n",
        "train_loader, val_loader, test_loader = create_dataloaders(data_config)\n",
        "\n",
        "# Create model\n",
        "print(\"\\n Creating model...\")\n",
        "model = UNet(n_channels=15, n_classes=3, bilinear=True)\n",
        "total_params, _ = model.count_parameters()\n",
        "print(f\"Model parameters: {total_params:,}\")\n",
        "\n",
        "# Try one batch\n",
        "inputs, targets, ids = next(iter(train_loader))\n",
        "print(f\"\\nBatch shapes: {inputs.shape} -> {targets.shape}\")\n",
        "\n",
        "# Start Training\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STARTING TRAINING\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "trained_model, history = train_model(model, train_loader, val_loader, train_config)\n",
        "\n",
        "print(\"ALL DONE!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kuKLqN0SQ3x"
      },
      "source": [
        "# 3D U-Net Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "nZZFB-bbSUl7"
      },
      "outputs": [],
      "source": [
        "class DoubleConv3D(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
        "        super().__init__()\n",
        "        if not mid_channels:\n",
        "            mid_channels = out_channels\n",
        "\n",
        "        self.double_conv = nn.Sequential(\n",
        "            nn.Conv3d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm3d(mid_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv3d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm3d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.double_conv(x)\n",
        "\n",
        "\n",
        "class Down3D(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        # Only downsample spatially (H, W), keep temporal dimension\n",
        "        self.maxpool_conv = nn.Sequential(\n",
        "            nn.MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2)),  # (T, H, W)\n",
        "            DoubleConv3D(in_channels, out_channels)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.maxpool_conv(x)\n",
        "\n",
        "\n",
        "class Up3D(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
        "        super().__init__()\n",
        "\n",
        "        if bilinear:\n",
        "            # Use trilinear interpolation for 3D\n",
        "            self.up = nn.Upsample(scale_factor=(1, 2, 2), mode='trilinear', align_corners=True)\n",
        "            self.conv = DoubleConv3D(in_channels, out_channels, in_channels // 2)\n",
        "        else:\n",
        "            # Transposed convolution - only upsample spatially\n",
        "            self.up = nn.ConvTranspose3d(\n",
        "                in_channels, in_channels // 2,\n",
        "                kernel_size=(1, 2, 2),\n",
        "                stride=(1, 2, 2)\n",
        "            )\n",
        "            self.conv = DoubleConv3D(in_channels, out_channels)\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        x1 = self.up(x1)\n",
        "\n",
        "        # Handle size mismatch (T, H, W)\n",
        "        diffT = x2.size()[2] - x1.size()[2]\n",
        "        diffY = x2.size()[3] - x1.size()[3]\n",
        "        diffX = x2.size()[4] - x1.size()[4]\n",
        "\n",
        "        x1 = F.pad(x1, [\n",
        "            diffX // 2, diffX - diffX // 2,\n",
        "            diffY // 2, diffY - diffY // 2,\n",
        "            diffT // 2, diffT - diffT // 2\n",
        "        ])\n",
        "\n",
        "        # Concatenate along channel dimension\n",
        "        x = torch.cat([x2, x1], dim=1)\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class UNet3D(nn.Module):\n",
        "\n",
        "    def __init__(self, n_frames=5, input_channels=3, output_channels=3, bilinear=True):\n",
        "        super().__init__()\n",
        "        self.n_frames = n_frames\n",
        "        self.input_channels = input_channels\n",
        "        self.output_channels = output_channels\n",
        "        self.bilinear = bilinear\n",
        "\n",
        "        self.inc = DoubleConv3D(input_channels, 64)\n",
        "\n",
        "        # Encoder (downsampling spatially)\n",
        "        self.down1 = Down3D(64, 128)\n",
        "        self.down2 = Down3D(128, 256)\n",
        "        self.down3 = Down3D(256, 512)\n",
        "        factor = 2 if bilinear else 1\n",
        "        self.down4 = Down3D(512, 1024 // factor)\n",
        "\n",
        "        # Decoder (upsampling spatially)\n",
        "        self.up1 = Up3D(1024, 512 // factor, bilinear)\n",
        "        self.up2 = Up3D(512, 256 // factor, bilinear)\n",
        "        self.up3 = Up3D(256, 128 // factor, bilinear)\n",
        "        self.up4 = Up3D(128, 64, bilinear)\n",
        "\n",
        "        # Final output layer\n",
        "        self.outc = nn.Conv3d(64, output_channels, kernel_size=(n_frames, 1, 1))\n",
        "\n",
        "        # Output activation\n",
        "        self.output_activation = nn.Tanh()\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        B, T, C, H, W = x.shape\n",
        "        x = x.permute(0, 2, 1, 3, 4)  # [B, 3, 5, H, W]\n",
        "\n",
        "        # Encoder with skip connections\n",
        "        x1 = self.inc(x)      # [B, 64, 5, H, W]\n",
        "        x2 = self.down1(x1)   # [B, 128, 5, H/2, W/2]\n",
        "        x3 = self.down2(x2)   # [B, 256, 5, H/4, W/4]\n",
        "        x4 = self.down3(x3)   # [B, 512, 5, H/8, W/8]\n",
        "        x5 = self.down4(x4)   # [B, 512, 5, H/16, W/16]\n",
        "\n",
        "        # Decoder with skip connections\n",
        "        x = self.up1(x5, x4)  # [B, 512, 5, H/8, W/8]\n",
        "        x = self.up2(x, x3)   # [B, 256, 5, H/4, W/4]\n",
        "        x = self.up3(x, x2)   # [B, 128, 5, H/2, W/2]\n",
        "        x = self.up4(x, x1)   # [B, 64, 5, H, W]\n",
        "\n",
        "        # Final convolution to reduce temporal dimension\n",
        "        logits = self.outc(x)  # [B, 3, 1, H, W]\n",
        "\n",
        "        # Remove temporal dimension\n",
        "        logits = logits.squeeze(2)  # [B, 3, H, W]\n",
        "\n",
        "        # Activation\n",
        "        output = self.output_activation(logits)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def count_parameters(self):\n",
        "        \"\"\"Count total and trainable parameters\"\"\"\n",
        "        total_params = sum(p.numel() for p in self.parameters())\n",
        "        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "        return total_params, trainable_params\n",
        "\n",
        "\n",
        "def test_unet3d():\n",
        "    \"\"\"Test the 3D U-Net model\"\"\"\n",
        "    print(\"=\" * 80)\n",
        "    print(\"Testing 3D U-Net Model\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    model = UNet3D(n_frames=5, input_channels=3, output_channels=3, bilinear=True)\n",
        "\n",
        "    total_params, trainable_params = model.count_parameters()\n",
        "    print(f\" Model Parameters:\")\n",
        "    print(f\"   - Total: {total_params:,}\")\n",
        "    print(f\"   - Trainable: {trainable_params:,}\")\n",
        "    print(f\"   - Size: ~{total_params * 4 / 1024 / 1024:.2f} MB\")\n",
        "\n",
        "    batch_size = 2\n",
        "    input_tensor = torch.randn(batch_size, 5, 3, 256, 256)\n",
        "\n",
        "    print(f\"Testing forward pass...\")\n",
        "    print(f\"   - Input shape: {input_tensor.shape}\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(input_tensor)\n",
        "\n",
        "    print(f\"   - Output shape: {output.shape}\")\n",
        "    print(f\"   - Output range: [{output.min():.3f}, {output.max():.3f}]\")\n",
        "\n",
        "    assert output.shape == (batch_size, 3, 256, 256), \"Output shape mismatch!\"\n",
        "\n",
        "    print(f\"3D U-Net test passed!\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "print(\"3D U-Net model defined!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b9HxIjtnSzpL",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "\n",
        "train_config = TrainConfig()\n",
        "\n",
        "# Create dataloaders\n",
        "print(\"Loading data...\")\n",
        "train_loader, val_loader, test_loader = create_dataloaders(data_config)\n",
        "\n",
        "# Create 3D U-Net model\n",
        "print(\"Creating 3D U-Net model...\")\n",
        "model = UNet3D(n_frames=5, input_channels=3, output_channels=3, bilinear=True)\n",
        "total_params, _ = model.count_parameters()\n",
        "print(f\"Model parameters: {total_params:,}\")\n",
        "\n",
        "# Try one batch\n",
        "inputs, targets, ids = next(iter(train_loader))\n",
        "print(f\"\\nBatch shapes: {inputs.shape} -> {targets.shape}\")\n",
        "\n",
        "# Start\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STARTING TRAINING WITH 3D U-NET\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "trained_model, history = train_model(model, train_loader, val_loader, train_config)\n",
        "\n",
        "print(\"ALL DONE!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6YhzUa7xKPc"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqd_AVYMNk7R"
      },
      "source": [
        "# ConvLSTM U-Net Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "UWzTC0B7NLRF"
      },
      "outputs": [],
      "source": [
        "class DoubleConv(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.double_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, 3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, 3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.double_conv(x)\n",
        "\n",
        "\n",
        "class Down(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.maxpool_conv = nn.Sequential(\n",
        "            nn.MaxPool2d(2),\n",
        "            DoubleConv(in_channels, out_channels)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.maxpool_conv(x)\n",
        "\n",
        "\n",
        "class Up(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
        "        super().__init__()\n",
        "        if bilinear:\n",
        "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "            self.conv = DoubleConv(in_channels, out_channels)\n",
        "        else:\n",
        "            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, 2, stride=2)\n",
        "            self.conv = DoubleConv(in_channels, out_channels)\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        x1 = self.up(x1)\n",
        "        # Handle size mismatch\n",
        "        diffY = x2.size()[2] - x1.size()[2]\n",
        "        diffX = x2.size()[3] - x1.size()[3]\n",
        "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2, diffY // 2, diffY - diffY // 2])\n",
        "        x = torch.cat([x2, x1], dim=1)\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class ConvLSTMCell(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, hidden_dim, kernel_size):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        padding = kernel_size // 2\n",
        "\n",
        "        self.conv = nn.Conv2d(\n",
        "            input_dim + hidden_dim,\n",
        "            4 * hidden_dim,  # Gates: input, forget, output, cell\n",
        "            kernel_size,\n",
        "            padding=padding\n",
        "        )\n",
        "\n",
        "    def forward(self, x, hidden_state):\n",
        "        h_cur, c_cur = hidden_state\n",
        "        combined = torch.cat([x, h_cur], dim=1)\n",
        "        gates = self.conv(combined)\n",
        "\n",
        "        i, f, o, g = torch.split(gates, self.hidden_dim, dim=1)\n",
        "        i = torch.tanh(i)\n",
        "        f = torch.tanh(f)\n",
        "        o = torch.tanh(o)\n",
        "        g = torch.tanh(g)\n",
        "\n",
        "        c_next = f * c_cur + i * g\n",
        "        h_next = o * torch.tanh(c_next)\n",
        "\n",
        "        return h_next, (h_next, c_next)\n",
        "\n",
        "\n",
        "class ConvLSTMUNet(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # Encoder\n",
        "        self.enc1 = DoubleConv(3, 64)\n",
        "        self.enc2 = Down(64, 128)\n",
        "        self.enc3 = Down(128, 256)\n",
        "\n",
        "        # ConvLSTM\n",
        "        self.convlstm = ConvLSTMCell(256, 256, kernel_size=3)\n",
        "\n",
        "        # Decoder\n",
        "        self.up1 = Up(256 + 256, 128, bilinear=True)  # 256 from ConvLSTM + 256 from skip\n",
        "        self.up2 = Up(128 + 128, 64, bilinear=True)  # 128 from up1 + 128 from skip\n",
        "        self.up3 = Up(64 + 64, 64, bilinear=True)   # 64 from up2 + 64 from skip\n",
        "        self.out = nn.Conv2d(64, 3, 1)\n",
        "        self.Tanh = nn.Tanh()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [B, T=5, C=3, H, W]\n",
        "        B, T, C, H, W = x.shape\n",
        "\n",
        "        # Encode each frame and store all features for skip connections\n",
        "        all_f1, all_f2, all_f3 = [], [], []\n",
        "\n",
        "        for t in range(T):\n",
        "            frame = x[:, t]          # [B, 3, H, W]\n",
        "            f1 = self.enc1(frame)       # [B, 64, H, W]\n",
        "            f2 = self.enc2(f1)         # [B, 128, H/2, W/2]\n",
        "            f3 = self.enc3(f2)         # [B, 256, H/4, W/4]\n",
        "            all_f1.append(f1)\n",
        "            all_f2.append(f2)\n",
        "            all_f3.append(f3)\n",
        "\n",
        "        # Process temporal sequence with ConvLSTM\n",
        "        h_size = (B, 256, H//4, W//4)\n",
        "        h = torch.zeros(h_size, device=x.device)\n",
        "        c = torch.zeros(h_size, device=x.device)\n",
        "\n",
        "        for t in range(T):\n",
        "            h, (h, c) = self.convlstm(all_f3[t], (h, c))\n",
        "\n",
        "        out = self.up1(h, all_f3[-1])     # [B, 128, H/2, W/2]\n",
        "        out = self.up2(out, all_f2[-1])    # [B, 64, H, W]\n",
        "        out = self.up3(out, all_f1[-1])    # [B, 64, H, W]\n",
        "        out = self.out(out)           # [B, 3, H, W]\n",
        "\n",
        "        return self.Tanh(out)\n",
        "\n",
        "    def count_parameters(self):\n",
        "        \"\"\"Count total and trainable parameters\"\"\"\n",
        "        total_params = sum(p.numel() for p in self.parameters())\n",
        "        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "        return total_params, trainable_params\n",
        "\n",
        "\n",
        "print(\"ConvLSTM U-Net model defined!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xor10GLZOdMX"
      },
      "outputs": [],
      "source": [
        "train_config = TrainConfig()\n",
        "\n",
        "# Create dataloaders\n",
        "print(\"Loading data...\")\n",
        "train_loader, val_loader, test_loader = create_dataloaders(data_config)\n",
        "\n",
        "# Create ConvLSTM U-Net model\n",
        "print(\"Creating ConvLSTM U-Net model...\")\n",
        "model = ConvLSTMUNet()\n",
        "total_params, _ = model.count_parameters()\n",
        "print(f\"Model parameters: {total_params:,}\")\n",
        "\n",
        "# Try one batch\n",
        "inputs, targets, ids = next(iter(train_loader))\n",
        "print(f\"\\nBatch shapes: {inputs.shape} -> {targets.shape}\")\n",
        "\n",
        "# Start\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STARTING TRAINING WITH CONVLSTM U-NET\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "trained_model, history = train_model(model, train_loader, val_loader, train_config)\n",
        "\n",
        "print(\"ALL DONE!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WQ46PdU4l5Y"
      },
      "source": [
        "# End of Training (Result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WUpQrAnAIjos"
      },
      "outputs": [],
      "source": [
        "plot_training_history(history, save_path=f\"{train_config.VIS_DIR}/training_curves.png\")\n",
        "\n",
        "import lpips\n",
        "# Load best checkpoint\n",
        "checkpoint = torch.load(f\"{train_config.SAVE_DIR}/checkpoint_best.pth\")\n",
        "trained_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "# Evaluate on test set\n",
        "print(\"\\n Evaluating on test set...\")\n",
        "device = train_config.DEVICE\n",
        "trained_model.eval()\n",
        "\n",
        "test_loss, test_psnr = 0, 0\n",
        "criterion = CombinedLoss(1.0, 1.0, 0.0, 0.002, 2.0, 1.0, device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, targets, _ in tqdm(test_loader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        outputs = trained_model(inputs)\n",
        "        loss, lpips, l1, perc, psnr, bg, color = criterion(outputs, targets)\n",
        "        psnr = calculate_psnr(outputs, targets)\n",
        "        test_loss += loss.item()\n",
        "        test_psnr += psnr.item()\n",
        "\n",
        "avg_test_loss = test_loss / len(test_loader)\n",
        "avg_test_psnr = test_psnr / len(test_loader)\n",
        "\n",
        "print(f\"\\n Test Results:\")\n",
        "print(f\"   Loss: {avg_test_loss:.4f}\")\n",
        "print(f\"   PSNR: {avg_test_psnr:.2f} dB\")\n",
        "\n",
        "def predict_single_plant(model, plant_id, data_config, device):\n",
        "    \"\"\"Predict growth for a single plant\"\"\"\n",
        "    dataset = PlantGrowthDataset(\n",
        "        data_config.DATA_ROOT, [plant_id],\n",
        "        data_config.INPUT_DAYS, data_config.TARGET_DAY,\n",
        "        data_config.IMG_SIZE\n",
        "    )\n",
        "\n",
        "    inputs, target, _ = dataset[0]\n",
        "    inputs = inputs.unsqueeze(0).to(device)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        prediction = model(inputs)\n",
        "\n",
        "    fig, axes = plt.subplots(1, 7, figsize=(21, 3))\n",
        "    for i in range(5):\n",
        "        img = denormalize(inputs[0,i]).permute(1,2,0).cpu().numpy()\n",
        "        axes[i].imshow(np.clip(img, 0, 1))\n",
        "        axes[i].set_title(f'Day {i+1}')\n",
        "        axes[i].axis('off')\n",
        "\n",
        "    pred = denormalize(prediction[0]).permute(1,2,0).cpu().numpy()\n",
        "    axes[5].imshow(np.clip(pred, 0, 1))\n",
        "    axes[5].set_title('Predicted Day 10', color='blue', fontweight='bold')\n",
        "    axes[5].axis('off')\n",
        "\n",
        "    gt = denormalize(target).permute(1,2,0).cpu().numpy()\n",
        "    axes[6].imshow(np.clip(gt, 0, 1))\n",
        "    axes[6].set_title('Ground Truth', color='green', fontweight='bold')\n",
        "    axes[6].axis('off')\n",
        "\n",
        "    plt.suptitle(f'Plant {plant_id:02d} Growth Prediction', fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Predict for plant 16, 21, 26, 27 ,34 ,5\n",
        "for i in [16, 21, 26, 27 ,34 ,5]:\n",
        "  predict_single_plant(trained_model, plant_id=i, data_config=data_config, device=train_config.DEVICE)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "wWI2TWdrmD_e",
        "aqYHhiBCmN1k",
        "x8byFGiMB3PQ",
        "aXgrpX8CCKX1",
        "Qd_Wa4xgOKSE",
        "3kuKLqN0SQ3x",
        "vqd_AVYMNk7R",
        "8WQ46PdU4l5Y"
      ],
      "gpuType": "A100",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}